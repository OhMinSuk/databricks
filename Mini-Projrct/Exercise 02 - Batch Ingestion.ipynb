{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3e20b18",
   "metadata": {},
   "source": [
    "# Databricks notebook source\n",
    "\n",
    "# 연습 #2 - 일괄 처리\n",
    "\n",
    "# MAGIC\n",
    "\n",
    "이 연습에서는 2017년, 2018년, 2019년 각각 하나씩, 총 세 개의 주문 일괄 처리를 처리합니다.\n",
    "\n",
    "# MAGIC\n",
    "\n",
    "각 일괄 처리를 처리할 때마다 새로운 델타 테이블에 추가하여 모든 데이터 세트를 하나의 단일 데이터 세트로 통합합니다.\n",
    "\n",
    "# MAGIC\n",
    "\n",
    "매년 다른 개인과 다른 표준이 사용되어 데이터 세트가 약간씩 달라졌습니다.\n",
    "\n",
    "* 2017년 백업은 고정 너비 텍스트 파일로 작성되었습니다.\n",
    "\n",
    "* 2018년 백업은 탭으로 구분된 텍스트 파일로 작성되었습니다.\n",
    "\n",
    "* 2019년 백업은 \"표준\" 쉼표로 구분된 텍스트 파일로 작성되었지만, 열 이름 형식은 변경되었습니다.\n",
    "\n",
    "# MAGIC\n",
    "\n",
    "여기서 우리의 유일한 목표는 모든 데이터 세트를 통합하는 동시에 추가 문제가 발생할 경우 각 레코드의 출처(처리된 파일 이름 및 처리된 타임스탬프)를 추적하는 것입니다.\n",
    "\n",
    "# MAGIC\n",
    "\n",
    "이 단계에서는 데이터 수집에만 집중하므로 대부분의 열은 단순 문자열로 수집되며, 향후 연습에서는 다양한 변환을 통해 이 문제(및 기타 문제)를 해결할 것입니다.\n",
    "\n",
    "# MAGIC\n",
    "\n",
    "진행하면서 몇 가지 \"현실 확인\"을 통해 제대로 진행되고 있는지 확인할 수 있습니다. 해당 솔루션을 구현한 후 해당 명령을 실행하기만 하면 됩니다.\n",
    "\n",
    "# MAGIC\n",
    "\n",
    "이 연습은 3단계로 나뉩니다.\n",
    "\n",
    "* 연습 2.A - 고정 너비 파일 데이터 수집\n",
    "\n",
    "* 연습 2.B - 탭으로 구분된 파일 데이터 수집\n",
    "\n",
    "* 연습 2.C - 쉼표로 구분된 파일 데이터 수집\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4b8eb5",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# MAGIC\n",
    "\n",
    "시작하려면 다음 셀을 실행하여 이 연습을 설정하고, 연습별 변수와 함수를 선언하세요.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833bb16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "%run ./_includes/Setup-Exercise-02\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5860b37",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ecf79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "files = dbutils.fs.ls(f\"{working_dir}/raw/orders/batch\") # List all the files\n",
    "\n",
    "display(files)                                           # Display the list of files\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fbeafa",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# MAGIC\n",
    "\n",
    "**이 단계에서는 다음 작업을 수행해야 합니다.**\n",
    "\n",
    "1. 필요한 경우 **`batch_2017_path`** 변수와 **`dbutils.fs.head`** 변수를 사용하여 2017 배치 파일을 조사합니다.\n",
    "\n",
    "2. **`batch_2017_path`**로 식별된 텍스트 파일을 수집하도록 **`DataFrameReader`**를 구성합니다. 이렇게 하면 줄당 하나의 레코드가 제공되고, **`value`**라는 단일 열이 있어야 합니다.\n",
    "\n",
    "3. **`fixed_width_column_defs`**(또는 사전 자체)의 정보를 사용하여 **`value`** 열을 사용하여 적절한 길이의 새 열을 추출합니다.<br/>\n",
    "\n",
    "* 사전의 키는 열 이름입니다.\n",
    "\n",
    "* 사전의 값에 있는 첫 번째 요소는 해당 열 데이터의 시작 위치입니다.\n",
    "\n",
    "* 사전의 값에 있는 두 번째 요소는 해당 열 데이터의 길이입니다.\n",
    "\n",
    "4. **`value`** 열에 대한 작업이 완료되면 제거합니다.\n",
    "\n",
    "5. 3단계에서 생성된 각 새 열에 대해 선행 공백을 제거합니다.\n",
    "\n",
    "* **`value`** 열에서 고정 너비 값을 추출할 때 \\[선행\\] 공백이 발생할 수 있습니다.\n",
    "\n",
    "6. 3단계에서 생성된 각 새 열에 대해 모든 빈 문자열을 **`null`**로 바꿉니다.\n",
    "\n",
    "* 공백을 제거한 후, 원본 데이터세트에 값이 지정되지 않은 열은 모두 빈 문자열이 됩니다.\n",
    "\n",
    "7. 데이터를 읽어온 파일 이름인 **`ingest_file_name`**이라는 새 열을 추가합니다.\n",
    "\n",
    "* 이 부분은 하드코딩되어서는 안 됩니다.\n",
    "\n",
    "* 해당 함수는 <a href=\"https://spark.apache.org/docs/latest/api/python/index.html\" target=\"_blank\">pyspark.sql.functions</a> 모듈을 참조하세요.\n",
    "\n",
    "8. 데이터가 DataFrame으로 수집된 타임스탬프인 **`ingested_at`**이라는 새 열을 추가합니다.\n",
    "\n",
    "* 이 부분은 하드코딩되어서는 안 됩니다.\n",
    "\n",
    "* 적절한 함수는 <a href=\"https://spark.apache.org/docs/latest/api/python/index.html\" target=\"_blank\">pyspark.sql.functions</a> 모듈을 참조하세요.\n",
    "\n",
    "9. 해당 **`DataFrame`**을 \"델타\" 형식으로 **`batch_target_path`**에 지정된 위치에 작성합니다.\n",
    "\n",
    "# MAGIC\n",
    "\n",
    "**참고 사항:**\n",
    "\n",
    "* **`fixed_width_column_defs`** 사전을 사용하여 프로그래밍 방식으로 각 열을 추출할 수 있지만, 이 단계를 하드 코딩하여 한 번에 한 열씩 추출해도 괜찮습니다.\n",
    "\n",
    "* **`SparkSession`**은 이미 **`spark`**의 인스턴스로 제공됩니다.\n",
    "\n",
    "* 이 연습에 필요한 클래스/메서드는 다음과 같습니다.\n",
    "\n",
    "* **`pyspark.sql.DataFrameReader`**: 데이터 수집\n",
    "\n",
    "* **`pyspark.sql.DataFrameWriter`**: 데이터 수집\n",
    "\n",
    "* **`pyspark.sql.Column`**: 데이터 변환\n",
    "\n",
    "* **`pyspark.sql.functions`** 모듈의 다양한 함수\n",
    "\n",
    "* **`pyspark.sql.DataFrame`**의 다양한 변환 및 작업\n",
    "\n",
    "* 다음 메서드를 사용하여 Databricks 파일 시스템(DBFS)을 조사하고 조작할 수 있습니다.\n",
    "\n",
    "* **`dbutils.fs.ls(..)`**: 파일 나열\n",
    "\n",
    "* **`dbutils.fs.rm(..)`**: 파일 제거\n",
    "\n",
    "* **`dbutils.fs.head(..)`**: 파일의 처음 N바이트 보기\n",
    "\n",
    "# MAGIC\n",
    "\n",
    "**추가 요구 사항:**\n",
    "\n",
    "* 통합 배치 데이터 세트는 \"델타\" 형식으로 디스크에 기록되어야 합니다.\n",
    "\n",
    "* 통합 배치 데이터 세트의 스키마는 다음과 같아야 합니다.\n",
    "\n",
    "  * **`submitted_at`**:**`string`**\n",
    "\n",
    "  * **`order_id`**:**`string`**\n",
    "\n",
    "  * **`customer_id`**:**`string`**\n",
    "\n",
    "  * **`sales_rep_id`**:**`string`**\n",
    "\n",
    "  * **`sales_rep_ssn`**:**`string`**\n",
    "\n",
    "  * **`sales_rep_first_name`**:**`string`**\n",
    "\n",
    "  * **`sales_rep_last_name`**:**`string`**\n",
    "\n",
    "  * **`sales_rep_address`**:**`string`**\n",
    "\n",
    "  * **`sales_rep_city`**:**`string`**\n",
    "\n",
    "  * **`sales_rep_state`**:**`string`**\n",
    "\n",
    "  * **`sales_rep_zip`**:**`string`**\n",
    "\n",
    "  * **`shipping_address_attention`**:**`string`**\n",
    "\n",
    "  * **`shipping_address_address`**:**`string`**\n",
    "\n",
    "  * **`shipping_address_city`**:**`string`**\n",
    "\n",
    "  * **`shipping_address_state`**:**`string`**\n",
    "\n",
    "  * **`shipping_address_zip`**:**`string`**\n",
    "\n",
    "  * **`product_id`**:**`string`**\n",
    "\n",
    "  * **`product_quantity`**:**`string`**\n",
    "\n",
    "  * **`product_sold_price`**:**`string`**\n",
    "\n",
    "  * **`ingest_file_name`**:**`string`**\n",
    "\n",
    "  * **`ingested_at`**:**`timestamp`**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293f6069",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# MAGIC\n",
    "\n",
    "다음 사전은 참조 및/또는 구현을 위해 제공됩니다.<br/>\n",
    "\n",
    "(선택한 전략에 따라 다름).\n",
    "\n",
    "# MAGIC\n",
    "\n",
    "다음 셀을 실행하여 인스턴스화하세요.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba45925",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fixed_width_column_defs = {\n",
    "\n",
    "  \"submitted_at\": (1, 15),\n",
    "\n",
    "  \"order_id\": (16, 40),\n",
    "\n",
    "  \"customer_id\": (56, 40),\n",
    "\n",
    "  \"sales_rep_id\": (96, 40),\n",
    "\n",
    "  \"sales_rep_ssn\": (136, 15),\n",
    "\n",
    "  \"sales_rep_first_name\": (151, 15),\n",
    "\n",
    "  \"sales_rep_last_name\": (166, 15),\n",
    "\n",
    "  \"sales_rep_address\": (181, 40),\n",
    "\n",
    "  \"sales_rep_city\": (221, 20),\n",
    "\n",
    "  \"sales_rep_state\": (241, 2),\n",
    "\n",
    "  \"sales_rep_zip\": (243, 5),\n",
    "\n",
    "  \"shipping_address_attention\": (248, 30),\n",
    "\n",
    "  \"shipping_address_address\": (278, 40),\n",
    "\n",
    "  \"shipping_address_city\": (318, 20),\n",
    "\n",
    "  \"shipping_address_state\": (338, 2),\n",
    "\n",
    "  \"shipping_address_zip\": (340, 5),\n",
    "\n",
    "  \"product_id\": (345, 40),\n",
    "\n",
    "  \"product_quantity\": (385, 5),\n",
    "\n",
    "  \"product_sold_price\": (390, 20)\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "print(fixed_width_column_defs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fb5648",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# MAGIC\n",
    "\n",
    "다음 셀에 해결책을 구현하세요.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93622685",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from pyspark.sql.functions import col, substring, trim, when, input_file_name, current_timestamp\n",
    "\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# TODO\n",
    "\n",
    "df_2017 = (spark\n",
    "\n",
    "           .read\n",
    "\n",
    "           .option('header', 'false')\n",
    "\n",
    "           .text(batch_2017_path))\n",
    "\n",
    "\n",
    "\n",
    "for col_name, (start, length) in fixed_width_column_defs.items():\n",
    "\n",
    "  df_2017 = df_2017.withColumn(col_name, substring(col(\"value\"), start, length)) \n",
    "\n",
    "\n",
    "\n",
    "df_2017 = df_2017.drop(\"value\")\n",
    "\n",
    "\n",
    "\n",
    "for col_name in df_2017.columns:\n",
    "\n",
    "  df_2017 = df_2017.withColumn(col_name, trim(col(col_name)))\n",
    "\n",
    "\n",
    "\n",
    "for col_name in df_2017.columns:\n",
    "\n",
    "  df_2017 = df_2017.withColumn(col_name, when(col(col_name) == \"\",None).otherwise(col(col_name)))\n",
    "\n",
    "\n",
    "\n",
    "df_2017 = df_2017.withColumn(\"ingest_file_name\", input_file_name())\n",
    "\n",
    "df_2017 = df_2017.withColumn(\"ingested_at\", current_timestamp())\n",
    "\n",
    "\n",
    "\n",
    "df_2017.write.format(\"delta\").mode(\"overwrite\").save(batch_target_path)\n",
    "\n",
    "# Use this cell to complete your solution\n",
    "\n",
    "display(df_2017)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05524e20",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Run the following command to ensure that you are on track:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492f4194",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "reality_check_02_a()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98e4874",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# MAGIC\n",
    "\n",
    "**이 단계에서는 다음을 수행해야 합니다.**\n",
    "\n",
    "1. 필요한 경우 **`batch_2018_path`** 변수와 **`dbutils.fs.head`** 변수를 사용하여 2018 배치 파일을 조사합니다.\n",
    "\n",
    "2. **`batch_2018_path`**로 식별된 탭으로 구분된 파일을 수집하도록 **`DataFrameReader`**를 구성합니다.\n",
    "\n",
    "3. 데이터를 읽어온 파일의 이름인 **`ingest_file_name`**이라는 새 열을 추가합니다. 이 이름은 하드 코딩되어서는 안 됩니다.\n",
    "\n",
    "4. 데이터가 DataFrame으로 수집된 시점의 타임스탬프인 새 열 **`ingested_at`**을 추가합니다. 이 값은 하드코딩되어서는 안 됩니다.\n",
    "\n",
    "5. 이전에 생성된 데이터세트의 **`batch_target_path`**에 지정된 값에 해당하는 **`DataFrame`**을 **추가**합니다.\n",
    "\n",
    "# MAGIC\n",
    "\n",
    "**추가 요구 사항**\n",
    "\n",
    "* CSV 파일에 있는 **\"null\"** 문자열은 SQL 값 **null**로 대체해야 합니다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d833231b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# MAGIC\n",
    "\n",
    "다음 셀에 해결책을 구현하세요.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38407853",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# TODO\n",
    "\n",
    "df_2018 = (spark\n",
    "\n",
    "           .read\n",
    "\n",
    "           .option('header', 'true')\n",
    "\n",
    "           .option(\"sep\",\"\\t\")\n",
    "\n",
    "           .csv(batch_2018_path))\n",
    "\n",
    "\n",
    "\n",
    "df_2018 = df_2018.withColumn(\"ingest_file_name\",input_file_name())\n",
    "\n",
    "df_2018 = df_2018.withColumn(\"ingested_at\",current_timestamp())\n",
    "\n",
    "\n",
    "\n",
    "for col_name in df_2018.columns:\n",
    "\n",
    "  df_2018 = df_2018.withColumn(col_name, when(col(col_name) == \"null\",None).otherwise(col(col_name)))\n",
    "\n",
    "\n",
    "\n",
    "df_2018.write.format(\"delta\").mode(\"append\").save(batch_target_path)\n",
    "\n",
    "# Use this cell to complete your solution\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2fe5e2",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "다음 명령을 실행하여 제대로 진행되고 있는지 확인하세요.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0101db32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "reality_check_02_b()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d74f5c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# MAGIC\n",
    "\n",
    "**이 단계에서는 다음을 수행해야 합니다.**\n",
    "\n",
    "1. 필요한 경우 **`batch_2019_path`** 변수와 **`dbutils.fs.head`** 변수를 사용하여 2019 배치 파일을 조사합니다.\n",
    "\n",
    "2. **`batch_2019_path`**로 식별된 쉼표로 구분된 파일을 수집하도록 **`DataFrameReader`**를 구성합니다.\n",
    "\n",
    "3. 데이터를 읽어온 파일의 이름인 **`ingest_file_name`**이라는 새 열을 추가합니다. 이 이름은 하드 코딩되어서는 안 됩니다.\n",
    "\n",
    "4. 데이터가 DataFrame으로 수집된 시점의 타임스탬프인 새 열 **`ingested_at`**을 추가합니다. 이 값은 하드 코딩되어서는 안 됩니다.\n",
    "\n",
    "5. 이전에 생성된 데이터세트의 **`batch_target_path`**에 지정된 타임스탬프에 해당 **`DataFrame`**을 **추가**합니다.<br/>\n",
    "\n",
    "# MAGIC\n",
    "\n",
    "참고: 이 데이터세트의 열 이름은 연습 #2.A에서 정의된 스키마에 맞게 업데이트해야 합니다. 이를 위한 몇 가지 전략이 있습니다.\n",
    "\n",
    "* 수집 시 이름을 변경하는 스키마를 제공합니다.\n",
    "\n",
    "* 한 번에 한 열씩 수동으로 이름을 바꿉니다.\n",
    "\n",
    "* **`fixed_width_column_defs`**를 사용하여 프로그래밍 방식으로 한 번에 한 열씩 이름을 바꿉니다.\n",
    "\n",
    "* **`DataFrame`** 클래스에서 제공하는 변환을 사용하여 한 번의 작업으로 모든 열의 이름을 바꿉니다.\n",
    "\n",
    "# MAGIC\n",
    "\n",
    "**추가 요구 사항**\n",
    "\n",
    "* CSV 파일의 **\"null\"** 문자열은 SQL 값 **null**로 대체해야 합니다.<br/>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99382155",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# MAGIC\n",
    "\n",
    "다음 셀에 해답을 구현하세요.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b95793",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# TODO\n",
    "\n",
    "df_2019 = (spark\n",
    "\n",
    "           .read\n",
    "\n",
    "           .option('header', 'true')\n",
    "\n",
    "           .option(\"sep\",\",\")\n",
    "\n",
    "           .csv(batch_2019_path))\n",
    "\n",
    "\n",
    "\n",
    "for new_col in fixed_width_column_defs:\n",
    "\n",
    "  for old_col in df_2019.columns:\n",
    "\n",
    "    if old_col.lower().replace(\"_\", \"\") == new_col.replace(\"_\", \"\"):\n",
    "\n",
    "      df_2019 = df_2019.withColumnRenamed(old_col,new_col)\n",
    "\n",
    "\n",
    "\n",
    "df_2019 = df_2019.withColumn(\"ingest_file_name\",input_file_name())\n",
    "\n",
    "df_2019 = df_2019.withColumn(\"ingested_at\",current_timestamp())\n",
    "\n",
    "\n",
    "\n",
    "for col_name in df_2019.columns:\n",
    "\n",
    "  df_2019 = df_2019.withColumn(col_name, when(col(col_name) == \"null\",None).otherwise(col(col_name)))\n",
    "\n",
    "\n",
    "\n",
    "df_2019.write.format(\"delta\").mode(\"append\").save(batch_target_path)\n",
    "\n",
    "\n",
    "\n",
    "display(df_2019)\n",
    "\n",
    "# Use this cell to complete your solution\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c563621",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "다음 명령을 실행하여 제대로 진행되고 있는지 확인하세요.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8825b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "reality_check_02_c()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1020080",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# MAGIC\n",
    "\n",
    "다음 명령을 실행하여 이 연습이 완료되었는지 확인하세요.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe42fa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "reality_check_02_final()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
